{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "from torch.utils.data import DataLoader\n",
    "from PIL import Image\n",
    "import random\n",
    "\n",
    "# --- Data Augmentation and Width Normalization ---\n",
    "\n",
    "class WidthNormalize(object):\n",
    "    \"\"\"Normalizes the width of an image to a target width.\"\"\"\n",
    "    def __init__(self, target_width, preserve_aspect_ratio=False):\n",
    "        self.target_width = target_width\n",
    "        self.preserve_aspect_ratio = preserve_aspect_ratio\n",
    "\n",
    "    def __call__(self, img):\n",
    "        w, h = img.size\n",
    "        if self.preserve_aspect_ratio:\n",
    "            new_h = int(h * (self.target_width / w))\n",
    "            return img.resize((self.target_width, new_h), Image.BICUBIC)\n",
    "        else:\n",
    "            return img.resize((self.target_width, h), Image.BICUBIC)\n",
    "\n",
    "class RandomDistort(object):\n",
    "    \"\"\"Applies random distortions (translation, scaling, rotation).\"\"\"\n",
    "    def __init__(self, max_translation=3, max_scaling=1.15, max_rotation=15, p=0.5):\n",
    "        self.max_translation = max_translation\n",
    "        self.max_scaling = max_scaling\n",
    "        self.max_rotation = max_rotation\n",
    "        self.p = p\n",
    "\n",
    "    def __call__(self, img):\n",
    "        if random.random() > self.p:\n",
    "            return img\n",
    "        w, h = img.size\n",
    "        tx = random.randint(-self.max_translation, self.max_translation)\n",
    "        ty = random.randint(-self.max_translation, self.max_translation)\n",
    "        img = transforms.functional.affine(img, angle=0, translate=(tx, ty), scale=1, shear=0)\n",
    "        scale = random.uniform(1 / self.max_scaling, self.max_scaling)\n",
    "        img = transforms.functional.affine(img, angle=0, translate=(0, 0), scale=scale, shear=0)\n",
    "        angle = random.uniform(-self.max_rotation, self.max_rotation)\n",
    "        img = transforms.functional.rotate(img, angle)\n",
    "        return img\n",
    "\n",
    "class PadToSize(object):\n",
    "    \"\"\"Pads an image to a specific size.\"\"\"\n",
    "    def __init__(self, target_size):\n",
    "        self.target_size = target_size\n",
    "\n",
    "    def __call__(self, img):\n",
    "        w, h = img.size\n",
    "        pad_width = self.target_size[0] - w\n",
    "        pad_height = self.target_size[1] - h\n",
    "\n",
    "        # Calculate padding for left, top, right, bottom\n",
    "        pad_left = pad_width // 2\n",
    "        pad_top = pad_height // 2\n",
    "        pad_right = pad_width - pad_left\n",
    "        pad_bottom = pad_height - pad_top\n",
    "\n",
    "        # Use PIL's ImageOps.expand for padding with a border color (0 for black)\n",
    "        return transforms.functional.pad(img, (pad_left, pad_top, pad_right, pad_bottom), padding_mode='constant', fill=0)\n",
    "\n",
    "\n",
    "def get_mnist_datasets(root='./data', width_targets=None, train_transform=None, test_transform=None):\n",
    "    \"\"\"Gets MNIST datasets, handling width normalization and padding.\"\"\"\n",
    "\n",
    "    # 1. Base transforms: Convert to tensor and normalize.\n",
    "    base_transform = transforms.Compose([\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize((0.1307,), (0.3081,))  # MNIST mean and std\n",
    "    ])\n",
    "\n",
    "    # 2. Define train_transform: Include PadToSize *after* custom transforms.\n",
    "    if train_transform is not None:\n",
    "        train_transform = transforms.Compose([\n",
    "            train_transform,\n",
    "            PadToSize((29, 29)),  # Pad to 29x29 *after* distortions/resizing.\n",
    "            base_transform,  # Convert to tensor and normalize.\n",
    "        ])\n",
    "    else:\n",
    "        train_transform = transforms.Compose([\n",
    "            PadToSize((29, 29)), # Pad to 29x29\n",
    "            base_transform\n",
    "        ])\n",
    "\n",
    "    # 3. Define test_transform:  Also include padding!\n",
    "    if test_transform is not None:\n",
    "        test_transform = transforms.Compose([\n",
    "            test_transform,\n",
    "            PadToSize((29, 29)), # Pad to 29x29\n",
    "            base_transform\n",
    "        ])\n",
    "    else:\n",
    "        test_transform = transforms.Compose([\n",
    "           PadToSize((29, 29)),  # Pad to 29x29\n",
    "            base_transform,\n",
    "        ])\n",
    "\n",
    "\n",
    "    # 4. Load/download the original MNIST dataset\n",
    "    original_train_dataset = torchvision.datasets.MNIST(root=root, train=True, download=True, transform=train_transform)\n",
    "    original_test_dataset = torchvision.datasets.MNIST(root=root, train=False, download=True, transform=test_transform)\n",
    "\n",
    "    if width_targets is None:\n",
    "        return original_train_dataset, original_test_dataset\n",
    "\n",
    "    # 5. Create width-normalized datasets (if requested)\n",
    "    train_datasets = [original_train_dataset]\n",
    "    test_datasets = [original_test_dataset]\n",
    "    for target_width in width_targets:\n",
    "        width_norm_train_transform = transforms.Compose([\n",
    "            WidthNormalize(target_width), # Normalise width\n",
    "            train_transform,  # Apply other augmentation transform\n",
    "        ])\n",
    "        width_norm_test_transform = transforms.Compose([\n",
    "            WidthNormalize(target_width),  # Normalize width\n",
    "            test_transform,  # Apply other test transforms\n",
    "        ])\n",
    "        train_datasets.append(torchvision.datasets.MNIST(root=root, train=True, download=False, transform=width_norm_train_transform))\n",
    "        test_datasets.append(torchvision.datasets.MNIST(root=root, train=False, download=False, transform=width_norm_test_transform))\n",
    "    return train_datasets, test_datasets\n",
    "\n",
    "\n",
    "# --- Model Definition (Single Column DNN) ---\n",
    "\n",
    "class MNIST_DNN(nn.Module):\n",
    "    \"\"\"Single-column DNN for MNIST, as described in the paper.\"\"\"\n",
    "    def __init__(self, num_classes=10):\n",
    "        super(MNIST_DNN, self).__init__()\n",
    "        # Convolutional layers\n",
    "        self.conv1 = nn.Conv2d(1, 20, kernel_size=4, stride=1, padding=0)  # No padding\n",
    "        self.relu1 = nn.ReLU()\n",
    "        self.pool1 = nn.MaxPool2d(kernel_size=2, stride=2)\n",
    "        self.conv2 = nn.Conv2d(20, 40, kernel_size=5, stride=1, padding=0)\n",
    "        self.relu2 = nn.ReLU()\n",
    "        self.pool2 = nn.MaxPool2d(kernel_size=3, stride=3)\n",
    "\n",
    "        # Calculate the size of the input to the fully connected layer.  This calculation is NOW correct:\n",
    "        # 29x29 input -> conv1(k=4,s=1,p=0) -> 26x26 -> pool1(k=2,s=2) -> 13x13\n",
    "        # 13x13 -> conv2(k=5,s=1,p=0) -> 9x9 -> pool2(k=3,s=3) -> 3x3\n",
    "        self.fc_input_size = 40 * 3 * 3  # 40 channels * 3 * 3\n",
    "\n",
    "        # Fully connected layers\n",
    "        self.fc1 = nn.Linear(self.fc_input_size, 150)\n",
    "        self.relu3 = nn.ReLU()\n",
    "        self.fc2 = nn.Linear(150, num_classes)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.pool1(self.relu1(self.conv1(x)))\n",
    "        x = self.pool2(self.relu2(self.conv2(x)))\n",
    "        x = x.view(-1, self.fc_input_size)  # Flatten\n",
    "        x = self.relu3(self.fc1(x))\n",
    "        x = self.fc2(x)\n",
    "        return x\n",
    "\n",
    "# --- Training Function ---\n",
    "\n",
    "def train_model(model, train_loader, criterion, optimizer, num_epochs=10, device='cpu'):\n",
    "    \"\"\"Trains the given model.\"\"\"\n",
    "    model.to(device)\n",
    "    model.train()\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        running_loss = 0.0\n",
    "        for i, (inputs, labels) in enumerate(train_loader):\n",
    "            inputs, labels = inputs.to(device), labels.to(device)\n",
    "\n",
    "            # --- Debug Print (Check input shape) ---\n",
    "            #print(f\"Shape of inputs BEFORE model: {inputs.shape}\")\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(inputs)\n",
    "            loss = criterion(outputs, labels)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            running_loss += loss.item()\n",
    "\n",
    "        print(f'Epoch {epoch+1}/{num_epochs}, Loss: {running_loss / len(train_loader):.4f}')\n",
    "    print('Finished Training')\n",
    "    return model\n",
    "\n",
    "# --- Evaluation Function ---\n",
    "\n",
    "def evaluate_model(model, test_loader, device='cpu'):\n",
    "    \"\"\"Evaluates the model on the test set.\"\"\"\n",
    "    model.to(device)\n",
    "    model.eval()\n",
    "    correct = 0\n",
    "    total = 0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for inputs, labels in test_loader:\n",
    "            inputs, labels = inputs.to(device), labels.to(device)\n",
    "            outputs = model(inputs)\n",
    "            _, predicted = torch.max(outputs.data, 1)\n",
    "            total += labels.size(0)\n",
    "            correct += (predicted == labels).sum().item()\n",
    "\n",
    "    accuracy = 100 * correct / total\n",
    "    print(f'Accuracy on the test set: {accuracy:.2f}%')\n",
    "    return accuracy\n",
    "\n",
    "# --- Main Script ---\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    # Hyperparameters\n",
    "    batch_size = 64\n",
    "    learning_rate = 0.001\n",
    "    num_epochs = 10\n",
    "    width_targets = [10, 14, 18]\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "    # Data Loading:  Crucially, use the corrected get_mnist_datasets\n",
    "    train_transforms = RandomDistort()\n",
    "    train_datasets, test_datasets = get_mnist_datasets(width_targets=width_targets, train_transform=train_transforms)\n",
    "\n",
    "\n",
    "    # Model Training (Single Column - Example)\n",
    "    trained_models = []\n",
    "    for i, train_dataset in enumerate(train_datasets):\n",
    "      print(f\"Training model for dataset {i + 1}...\")\n",
    "      train_dataloader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "      test_dataloader = DataLoader(test_datasets[i], batch_size=batch_size, shuffle=False)\n",
    "\n",
    "      model = MNIST_DNN().to(device)\n",
    "      optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
    "      criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "      lambda_annealing = lambda epoch: 0.993**epoch\n",
    "      scheduler = torch.optim.lr_scheduler.LambdaLR(optimizer, lr_lambda=lambda_annealing)\n",
    "\n",
    "      model = train_model(model, train_dataloader, criterion, optimizer, num_epochs, device)\n",
    "      accuracy = evaluate_model(model, test_dataloader, device)\n",
    "      trained_models.append(model)\n",
    "      \n",
    "    # --- MCDNN Prediction (Averaging) ---\n",
    "    def predict_mcdnn(models, test_loader, device):\n",
    "      \"\"\"Predicts using the MCDNN ensemble.\"\"\"\n",
    "      all_predictions = []  # Store predictions from all models\n",
    "      all_labels_list = []\n",
    "\n",
    "      for model in models:\n",
    "          model.to(device)\n",
    "          model.eval()  # Set models to evaluation mode\n",
    "          predictions = [] # For storing individual model prediction\n",
    "          with torch.no_grad():\n",
    "              for inputs, labels in test_loader:\n",
    "                  inputs = inputs.to(device)\n",
    "                  outputs = model(inputs)\n",
    "                  # Apply softmax to get probabilities (not needed for training, but here we need prob)\n",
    "                  probabilities = torch.nn.functional.softmax(outputs, dim=1)\n",
    "                  predictions.append(probabilities.cpu())  # Move to CPU to avoid GPU memory issues.\n",
    "          all_predictions.append(torch.cat(predictions))  # Concatenate the predictions from all the batches.\n",
    "          all_labels_list.append(labels.cpu())\n",
    "          \n",
    "      # all_predictions is now a LIST of tensors.  Each tensor is the predictions of one model\n",
    "      # on the entire test set.  We need to average these *across models*.\n",
    "      all_predictions = torch.stack(all_predictions)  # Stack into a single tensor: [num_models, num_samples, num_classes]\n",
    "      averaged_predictions = torch.mean(all_predictions, dim=0) # Average across models (dim=0).  Result: [num_samples, num_classes]\n",
    "\n",
    "      # Get predicted classes from averaged probabilities\n",
    "      _, predicted_classes = torch.max(averaged_predictions, 1)  # Get the class with highest probability\n",
    "      return predicted_classes, torch.cat(all_labels_list)\n",
    "      \n",
    "    # Get MCDNN predictions on original test loader and calculate final accuracy.\n",
    "    final_test_loader = DataLoader(test_datasets[0], batch_size=batch_size, shuffle=False) #original test loader\n",
    "    mcdnn_predictions, final_test_labels = predict_mcdnn(trained_models, final_test_loader, device)\n",
    "\n",
    "    # Calculate accuracy\n",
    "    correct = (mcdnn_predictions == final_test_labels).sum().item()\n",
    "    total = final_test_labels.size(0)\n",
    "    mcdnn_accuracy = 100 * correct / total\n",
    "    print(f'MCDNN Accuracy on the original test set: {mcdnn_accuracy:.2f}%')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
