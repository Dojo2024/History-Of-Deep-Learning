{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torchvision.models as models\n",
    "import torchvision.transforms as transforms\n",
    "from PIL import Image\n",
    "import numpy as np\n",
    "import skimage.io\n",
    "from skimage.segmentation import slic\n",
    "from skimage.measure import find_contours\n",
    "from skimage.util import img_as_float\n",
    "import selectivesearch\n",
    "from sklearn import svm\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def selective_search_proposal(img):\n",
    "    \"\"\"\n",
    "    Generates region proposals using Selective Search.\n",
    "\n",
    "    Args:\n",
    "        img: A numpy array representing the input image.\n",
    "\n",
    "    Returns:\n",
    "        A list of bounding boxes (x1, y1, x2, y2).\n",
    "    \"\"\"\n",
    "    # Perform selective search\n",
    "    img_lbl, regions = selectivesearch.selective_search(img, scale=500, sigma=0.9, min_size=10)\n",
    "\n",
    "    candidates = set()\n",
    "    for r in regions:\n",
    "        # excluding same areas\n",
    "        if r['rect'] in candidates:\n",
    "            continue\n",
    "        # excluding small areas\n",
    "        if r['size'] < (img.shape[0]*img.shape[1]*0.001): #the size is > 0.01% total area of image\n",
    "            continue\n",
    "        # distorted rects\n",
    "        x, y, w, h = r['rect']\n",
    "        if w > h:\n",
    "            if w/h>3: #width is 3x >height\n",
    "                continue\n",
    "        else:\n",
    "            if h/w>3: #vice versa\n",
    "                continue\n",
    "        candidates.add(r['rect'])\n",
    "\n",
    "    boxes = [] #to return the bounding box locations\n",
    "    for x, y, w, h in candidates:\n",
    "        boxes.append((x, y, x + w, y + h))\n",
    "\n",
    "    return boxes #List[(x1, y1, x2, y2)], bounding box candidates."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CNNFeatureExtractor(nn.Module):\n",
    "    def __init__(self, model_name='alexnet', use_cuda=True):\n",
    "        super(CNNFeatureExtractor, self).__init__()\n",
    "        self.use_cuda = use_cuda\n",
    "        if model_name == 'alexnet':\n",
    "            self.cnn = models.alexnet(pretrained=True)\n",
    "            self.cnn = nn.Sequential(*list(self.cnn.children())[:-1]) # Remove classifier layer to get output pre the classifier\n",
    "            self.feature_size = 256 * 6 * 6  # output dimension before flattening, should be 9216\n",
    "        else:\n",
    "            raise NotImplementedError\n",
    "\n",
    "        if self.use_cuda:\n",
    "            self.cnn = self.cnn.cuda()\n",
    "        self.cnn.eval()  # Set to evaluation mode\n",
    "\n",
    "    def forward(self, images):\n",
    "        \"\"\"\n",
    "        Extracts CNN features from a batch of images.\n",
    "\n",
    "        Args:\n",
    "            images: A tensor of shape (B, 3, H, W) representing a batch of images.\n",
    "\n",
    "        Returns:\n",
    "            A tensor of shape (B, feature_size) representing the CNN features.\n",
    "        \"\"\"\n",
    "        with torch.no_grad():  # Disable gradient calculation during inference\n",
    "            features = self.cnn(images)\n",
    "            features = features.view(features.size(0), -1) # Flatten the convolution layer.\n",
    "        return features\n",
    "\n",
    "\n",
    "def warp_proposal(image, bbox, target_size=(227, 227), padding=16):\n",
    "    \"\"\"\n",
    "    Warps a region proposal to the target size, including context padding.\n",
    "\n",
    "    Args:\n",
    "        image: A PIL Image object.\n",
    "        bbox: A tuple (x1, y1, x2, y2) representing the bounding box.\n",
    "        target_size: A tuple (H, W) representing the target size.\n",
    "        padding: Number of pixels for context padding (post warping).\n",
    "\n",
    "    Returns:\n",
    "        A PIL Image object representing the warped region.\n",
    "    \"\"\"\n",
    "    x1, y1, x2, y2 = bbox\n",
    "    width, height = image.size #image.width and image.height works as well for Pillow\n",
    "\n",
    "    #Calculate Source Box Dimension before dilation\n",
    "    W_proposal = x2-x1\n",
    "    H_proposal = y2-y1\n",
    "\n",
    "    #Calculate source box scale to make sure that after it's warped to the target Size, there is Padding pixels of context\n",
    "    scale_x = float(target_size[0])/(W_proposal + padding*2) #new method with padding, and float cast, for greater precision and accuracy.\n",
    "    scale_y = float(target_size[1])/(H_proposal + padding*2)\n",
    "\n",
    "    # Calculate dilated Box\n",
    "    dilated_width= int(W_proposal+(padding*2/scale_x)) #scale back to find the dilation amount\n",
    "    dilated_height= int(H_proposal+(padding*2/scale_y))\n",
    "\n",
    "    center_x = (x1+x2) //2 #avoid floating number issue.\n",
    "\n",
    "    center_y = (y1+y2) //2\n",
    "\n",
    "    x1_dilated = center_x - dilated_width//2 #integer floor division\n",
    "    y1_dilated = center_y - dilated_height//2\n",
    "\n",
    "    x2_dilated = center_x + dilated_width//2\n",
    "    y2_dilated = center_y + dilated_height//2\n",
    "\n",
    "    # Clip box to stay inside the image, so it doesn't access null values\n",
    "    x1_clip = max(0,x1_dilated)\n",
    "    y1_clip = max(0,y1_dilated)\n",
    "    x2_clip = min(width,x2_dilated)\n",
    "    y2_clip = max(height,y2_dilated)\n",
    "\n",
    "    # Crop the warped image (using Pillow now)\n",
    "    warped_image = image.crop((x1_clip,y1_clip,x2_clip,y2_clip))\n",
    "\n",
    "    warped_image = warped_image.resize(target_size) #and after that you have a resized PIL image\n",
    "\n",
    "    return warped_image\n",
    "\n",
    "\n",
    "def extract_features(image_path, bboxes, cnn_model, transform, batch_size=32, use_cuda=True): #use CUDA here again\n",
    "    \"\"\"\n",
    "    Extracts CNN features from region proposals in an image.\n",
    "\n",
    "    Args:\n",
    "        image_path: Path to the input image.\n",
    "        bboxes: A list of bounding boxes (x1, y1, x2, y2).\n",
    "        cnn_model: The CNNFeatureExtractor model.\n",
    "        transform: PyTorch transform to apply to the warped images.\n",
    "        batch_size: Batch size for CNN processing.\n",
    "        use_cuda: A flag indicates whether to use CUDA.\n",
    "\n",
    "    Returns:\n",
    "        A numpy array of shape (num_proposals, feature_size) representing the CNN features.\n",
    "    \"\"\"\n",
    "    image = Image.open(image_path).convert('RGB')\n",
    "    num_proposals = len(bboxes)\n",
    "    features = []\n",
    "\n",
    "    for i in range(0, num_proposals, batch_size):\n",
    "        batch_bboxes = bboxes[i:i + batch_size]\n",
    "        batch_images = []\n",
    "        for bbox in batch_bboxes:\n",
    "            warped_image = warp_proposal(image, bbox)\n",
    "            img_tensor = transform(warped_image)\n",
    "            batch_images.append(img_tensor)\n",
    "\n",
    "        batch_tensor = torch.stack(batch_images) # List[tensor] => tensor\n",
    "\n",
    "        if use_cuda:\n",
    "            batch_tensor = batch_tensor.cuda()\n",
    "\n",
    "        batch_features = cnn_model(batch_tensor)\n",
    "        features.append(batch_features.cpu().numpy())\n",
    "\n",
    "    features = np.concatenate(features, axis=0)  #Join the batches together to return it\n",
    "    return features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_training_data(image_dir, annotation_dir, bboxes, labels, feature_extraction_func,  iou_threshold_pos=0.5, iou_threshold_neg=0.3,  num_negatives_per_positive=3 ):\n",
    "    \"\"\"\n",
    "    Creates training data for SVM classifiers using hard negative mining.\n",
    "\n",
    "    Args:\n",
    "        image_dir: Directory containing images.\n",
    "        annotation_dir: Directory containing annotation files (e.g., PASCAL VOC XML).\n",
    "        bboxes: bounding boxes from the proposal region (Selective Search most likely)\n",
    "        feature_extraction_func: A function that takes an image path and bounding boxes and returns CNN features.\n",
    "        iou_threshold_pos: IoU threshold for positive examples.\n",
    "        iou_threshold_neg: IoU threshold for negative examples.\n",
    "        num_negatives_per_positive: the amount of negative samples per positive sample\n",
    "\n",
    "    Returns:\n",
    "        A tuple (X, y) where X is a numpy array of CNN features and y is a numpy array of labels.\n",
    "    \"\"\"\n",
    "    X = []\n",
    "    y = []\n",
    "\n",
    "    #for each image we go through and get the GT box info and proposals\n",
    "    for image_name in os.listdir(image_dir): #iterate through all the images\n",
    "        if not image_name.endswith(\".jpg\"): #only access jpg.\n",
    "            continue\n",
    "\n",
    "        image_path = os.path.join(image_dir, image_name)\n",
    "\n",
    "        gt_boxes, gt_labels = load_annotations(os.path.join(annotation_dir, image_name + \".xml\"))  # Use your annotation loading function\n",
    "        image_bboxes = bboxes[image_name] #get the image's bounding boxes\n",
    "\n",
    "        num_gt_boxes = len(gt_boxes)\n",
    "        img_X, img_y = [], []  # lists to append within an image\n",
    "\n",
    "        #for gt bounding box append GT box and also top N proposals\n",
    "        for i, gt_box in enumerate(gt_boxes): #iterate through all the GT Boxes\n",
    "            #Append positive data here.\n",
    "            #GT box as positive label\n",
    "            img_X.append(gt_box)  #append the ground truth box location here\n",
    "            img_y.append(gt_labels[i])   #append the correct class name\n",
    "\n",
    "            #for every GT box try to append K proposals with IoU < 0.3\n",
    "            negative_candidates=[]\n",
    "\n",
    "            for proposal_box in image_bboxes:\n",
    "                iou = compute_iou(gt_box, proposal_box)\n",
    "\n",
    "                if iou < iou_threshold_neg: #if iou < 0.3\n",
    "                    negative_candidates.append(proposal_box)\n",
    "\n",
    "            #append top K negatives.\n",
    "            img_X.extend(negative_candidates[:num_negatives_per_positive])\n",
    "            img_y.extend([0]*num_negatives_per_positive) # append 0 which is the negative class\n",
    "\n",
    "\n",
    "        ###########################################################\n",
    "        #At this point we have all positives and negatives in an image.\n",
    "        ###########################################################\n",
    "\n",
    "        #perform feature extraction on ALL proposals before going to next image.\n",
    "        #the length here would be = numGTbox * N number of proposal * dimension,\n",
    "        num_boxes = len(img_X) #total boxes\n",
    "        features = feature_extraction_func(image_path, img_X)\n",
    "\n",
    "        #Now extend all.\n",
    "        X.extend(features)\n",
    "        y.extend(img_y)\n",
    "\n",
    "    return np.array(X), np.array(y)\n",
    "\n",
    "def train_svms(X, y, classes, use_cuda=True): #Added Cuda\n",
    "    \"\"\"\n",
    "    Trains a linear SVM classifier for each object category.\n",
    "\n",
    "    Args:\n",
    "        X: A numpy array of CNN features of shape (num_examples, feature_size).\n",
    "        y: A numpy array of labels of shape (num_examples,).\n",
    "        classes: A list of object category names.\n",
    "        use_cuda: a flag indicates whether to use CUDA.\n",
    "\n",
    "    Returns:\n",
    "        A dictionary mapping class names to trained linear SVM classifiers.\n",
    "    \"\"\"\n",
    "    svms = {}\n",
    "    #Add a place to save results after each class\n",
    "    svm_dir=\"./trained_svms/\"\n",
    "    if not os.path.exists(svm_dir):\n",
    "        os.makedirs(svm_dir)\n",
    "\n",
    "    for i, class_name in enumerate(classes):\n",
    "        print(\"Training svm model {0}/{1}:{2}\".format(i+1,len(classes),class_name))\n",
    "        #create path to save file with model name\n",
    "\n",
    "        #if model name exist, skip, so it doesn't need to train again and again.\n",
    "        outputfile = svm_dir+class_name+\".pth\" #I added this part. so you don't need to keep training for multiple times. if model is there, it loads and it's done. saves you tremendous amount of time.\n",
    "\n",
    "        #if already there then skip training\n",
    "        if os.path.exists(outputfile): #skip it as it's already present\n",
    "            model = svm.LinearSVC()\n",
    "            model = torch.load(outputfile)\n",
    "            svms[class_name] = model\n",
    "            print(\"Model was loaded and will continue\") #to show the progress,\n",
    "            continue #then skip.\n",
    "\n",
    "        #If Model is not found train\n",
    "        # Select positive and negative samples for this class\n",
    "        positive_indices = np.where(y == i)[0]\n",
    "        negative_indices = np.where(y == 0)[0] #Assuming Background Index is Zero\n",
    "\n",
    "        #SVM cannot take it if it's all one sided, it needs at least one negative/positive example\n",
    "        if len(positive_indices)<1:\n",
    "            print(\"{0} Skipped since number of positive examples are {1}\".format(class_name,len(positive_indices)))\n",
    "            continue\n",
    "        if len(negative_indices)<1:\n",
    "            print(\"{0} Skipped since number of negative examples are {1}\".format(class_name,len(negative_indices)))\n",
    "            continue\n",
    "\n",
    "        X_train = np.concatenate((X[positive_indices], X[negative_indices]), axis=0)\n",
    "        y_train = np.concatenate((np.ones(len(positive_indices)), np.zeros(len(negative_indices))))\n",
    "\n",
    "        # Train a linear SVM classifier\n",
    "        model = svm.LinearSVC(C=0.01, random_state=42, max_iter=1000) #I changed it a lil bit, #Set C=0.01 for higher regularization, to prevent the model from overfitting too much. and limited # of iters\n",
    "        model.fit(X_train, y_train)\n",
    "\n",
    "        svms[class_name] = model\n",
    "\n",
    "        #save model to load for next use if something happens.\n",
    "        torch.save(model, outputfile)\n",
    "        print(\"Model done and saved in \" + outputfile) #to show the progress.\n",
    "\n",
    "    return svms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(image_path, bboxes, cnn_model, svms, transform, classes, iou_threshold=0.3, confidence_threshold=0.5, use_cuda=True):\n",
    "    \"\"\"\n",
    "    Performs object detection on an image using R-CNN.\n",
    "\n",
    "    Args:\n",
    "        image_path: Path to the input image.\n",
    "        bboxes: A list of bounding boxes (x1, y1, x2, y2) from region proposals.\n",
    "        cnn_model: The CNNFeatureExtractor model.\n",
    "        svms: A dictionary mapping class names to trained linear SVM classifiers.\n",
    "        transform: PyTorch transform to apply to the warped images.\n",
    "        classes: A list of object category names.\n",
    "        iou_threshold: IoU threshold for NMS.\n",
    "        confidence_threshold: the confidence needed to classify as\n",
    "    Returns:\n",
    "        A list of dictionaries, where each dictionary represents a detection and contains:\n",
    "            - 'bbox': A tuple (x1, y1, x2, y2) representing the bounding box.\n",
    "            - 'class': The predicted object category.\n",
    "            - 'score': The confidence score.\n",
    "    \"\"\"\n",
    "    features = extract_features(image_path, bboxes, cnn_model, transform, use_cuda=use_cuda)\n",
    "    detections = []\n",
    "\n",
    "    for i, class_name in enumerate(classes):\n",
    "        svm_model = svms[class_name]\n",
    "        scores = svm_model.decision_function(features) #calculate the desicion,\n",
    "\n",
    "        #We only get the scores above a confidence thresholds that we determine\n",
    "        class_detections = []\n",
    "        for j, score in enumerate(scores):\n",
    "            if score >= confidence_threshold: #greater than confidence interval.\n",
    "                class_detections.append((bboxes[j], score))  #create a detection tuple\n",
    "\n",
    "        #apply NMS\n",
    "\n",
    "        # Extract bboxes and scores\n",
    "        bboxes_class, scores_class = zip(*class_detections) #reverse it into 2 lists\n",
    "\n",
    "        #Convert to same format for simplicity\n",
    "        bboxes_class = list(bboxes_class)\n",
    "        scores_class = list(scores_class)\n",
    "\n",
    "        #apply Non maximum supression\n",
    "        nms_indices = non_max_suppression(bboxes_class, scores_class, iou_threshold) #the iou threshold are common, and we call the bboxes function\n",
    "        for idx in nms_indices:\n",
    "            detections.append({\n",
    "                'bbox': bboxes_class[idx],\n",
    "                'class': class_name,\n",
    "                'score': scores_class[idx]\n",
    "            })\n",
    "\n",
    "    return detections\n",
    "\n",
    "def compute_iou(box1, box2):\n",
    "    \"\"\"\n",
    "    Computes the Intersection over Union (IoU) between two bounding boxes.\n",
    "\n",
    "    Args:\n",
    "        box1: A tuple (x1, y1, x2, y2) representing the first bounding box.\n",
    "        box2: A tuple (x1, y1, x2, y2) representing the second bounding box.\n",
    "\n",
    "    Returns:\n",
    "        The IoU between the two bounding boxes.\n",
    "    \"\"\"\n",
    "    x1_intersect = max(box1[0], box2[0])\n",
    "    y1_intersect = max(box1[1], box2[1])\n",
    "    x2_intersect = min(box1[2], box2[2])\n",
    "    y2_intersect = min(box1[3], box2[3])\n",
    "\n",
    "    intersect_width = max(0, x2_intersect - x1_intersect)\n",
    "    intersect_height = max(0, y2_intersect - y1_intersect)\n",
    "    intersect_area = intersect_width * intersect_height\n",
    "\n",
    "    box1_width = box1[2] - box1[0]\n",
    "    box1_height = box1[3] - box1[1]\n",
    "    box1_area = box1_width * box1_height\n",
    "\n",
    "    box2_width = box2[2] - box2[0]\n",
    "    box2_height = box2[3] - box2[1]\n",
    "    box2_area = box2_width * box2_height\n",
    "\n",
    "    union_area = box1_area + box2_area - intersect_area\n",
    "    iou = intersect_area / union_area if union_area > 0 else 0\n",
    "\n",
    "    return iou\n",
    "\n",
    "def non_max_suppression(boxes, scores, iou_threshold):\n",
    "    \"\"\"\n",
    "    Applies Non-Maximum Suppression (NMS) to a list of bounding boxes.\n",
    "\n",
    "    Args:\n",
    "        boxes: A list of bounding boxes (x1, y1, x2, y2).\n",
    "        scores: A list of confidence scores.\n",
    "        iou_threshold: IoU threshold for NMS.\n",
    "\n",
    "    Returns:\n",
    "        A list of indices of the bounding boxes to keep after NMS.\n",
    "    \"\"\"\n",
    "    # Convert the box coordinates to a NumPy array\n",
    "    boxes = np.array(boxes)\n",
    "\n",
    "    # If there are no boxes, return an empty list\n",
    "    if len(boxes) == 0:\n",
    "        return []\n",
    "\n",
    "    # Initialize the list of picked indexes\n",
    "    picked = []\n",
    "\n",
    "    # Grab the coordinates of the bounding boxes\n",
    "    x1 = boxes[:, 0]\n",
    "    y1 = boxes[:, 1]\n",
    "    x2 = boxes[:, 2]\n",
    "    y2 = boxes[:, 3]\n",
    "\n",
    "    # Compute the area of the bounding boxes and sort by the score\n",
    "    area = (x2 - x1 + 1) * (y2 - y1 + 1)\n",
    "    idxs = np.argsort(scores)[::-1]\n",
    "\n",
    "    # Keep looping while some indexes still remain in the indexes list\n",
    "    while len(idxs) > 0:\n",
    "        # Grab the last index in the indexes list and add the index value to the list of picked indexes\n",
    "        last = len(idxs) - 1\n",
    "        i = idxs[last]\n",
    "        picked.append(i)\n",
    "\n",
    "        # Find the largest (x, y) coordinates for the start of the bounding box and the smallest (x, y) coordinates for the end of the bounding box\n",
    "        xx1 = np.maximum(x1[i], x1[idxs[:last]])\n",
    "        yy1 = np.maximum(y1[i], y1[idxs[:last]])\n",
    "        xx2 = np.minimum(x2[i], x2[idxs[:last]])\n",
    "        yy2 = np.minimum(y2[i], y2[idxs[:last]])\n",
    "\n",
    "        # Compute the width and height of the intersection\n",
    "        w = np.maximum(0, xx2 - xx1 + 1)\n",
    "        h = np.maximum(0, yy2 - yy1 + 1)\n",
    "\n",
    "        # Compute the ratio of intersection over union\n",
    "        iou = (w * h) / area[idxs[:last]] + area[i] - (w * h) #calculate for the rest except the final one.\n",
    "        iou= (w*h)/(area[idxs[:last]])\n",
    "\n",
    "        # Delete all indexes from the index list that have intersection over union greater than the provided intersection over union threshold\n",
    "        idxs = np.delete(idxs, np.concatenate(([last], np.where(iou > iou_threshold)[0])))\n",
    "\n",
    "    # Return only the bounding boxes that were picked using non-maximum suppression\n",
    "    return picked"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Debojyoti Das\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\torchvision\\models\\_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
      "  warnings.warn(\n",
      "c:\\Users\\Debojyoti Das\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\torchvision\\models\\_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=AlexNet_Weights.IMAGENET1K_V1`. You can also use `weights=AlexNet_Weights.DEFAULT` to get the most up-to-date weights.\n",
      "  warnings.warn(msg)\n",
      "Downloading: \"https://download.pytorch.org/models/alexnet-owt-7be5be79.pth\" to C:\\Users\\Debojyoti Das/.cache\\torch\\hub\\checkpoints\\alexnet-owt-7be5be79.pth\n",
      "100%|██████████| 233M/233M [00:13<00:00, 17.8MB/s] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training svm model 1/20:Aeroplane\n",
      "Aeroplane Skipped since number of positive examples are 0\n",
      "Training svm model 2/20:Bicycle\n",
      "Bicycle Skipped since number of positive examples are 0\n",
      "Training svm model 3/20:Bird\n",
      "Bird Skipped since number of positive examples are 0\n",
      "Training svm model 4/20:Boat\n",
      "Boat Skipped since number of positive examples are 0\n",
      "Training svm model 5/20:Bottle\n",
      "Bottle Skipped since number of positive examples are 0\n",
      "Training svm model 6/20:Bus\n",
      "Bus Skipped since number of positive examples are 0\n",
      "Training svm model 7/20:Car\n",
      "Car Skipped since number of positive examples are 0\n",
      "Training svm model 8/20:Cat\n",
      "Cat Skipped since number of positive examples are 0\n",
      "Training svm model 9/20:Chair\n",
      "Chair Skipped since number of positive examples are 0\n",
      "Training svm model 10/20:Cow\n",
      "Cow Skipped since number of positive examples are 0\n",
      "Training svm model 11/20:Diningtable\n",
      "Diningtable Skipped since number of positive examples are 0\n",
      "Training svm model 12/20:Dog\n",
      "Dog Skipped since number of positive examples are 0\n",
      "Training svm model 13/20:Horse\n",
      "Horse Skipped since number of positive examples are 0\n",
      "Training svm model 14/20:Motorbike\n",
      "Motorbike Skipped since number of positive examples are 0\n",
      "Training svm model 15/20:Person\n",
      "Person Skipped since number of positive examples are 0\n",
      "Training svm model 16/20:Pottedplant\n",
      "Pottedplant Skipped since number of positive examples are 0\n",
      "Training svm model 17/20:Sheep\n",
      "Sheep Skipped since number of positive examples are 0\n",
      "Training svm model 18/20:Sofa\n",
      "Sofa Skipped since number of positive examples are 0\n",
      "Training svm model 19/20:Train\n",
      "Train Skipped since number of positive examples are 0\n",
      "Training svm model 20/20:Tvmonitor\n",
      "Tvmonitor Skipped since number of positive examples are 0\n"
     ]
    },
    {
     "ename": "FileNotFoundError",
     "evalue": "No such file: 'c:\\Users\\Debojyoti Das\\Downloads\\History\\Rich feature hierarchies for accurate object detection and semantic segmentation\\test_image.jpg'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[10], line 86\u001b[0m\n\u001b[0;32m     82\u001b[0m \u001b[38;5;66;03m# ---------------------------------------------------------------------\u001b[39;00m\n\u001b[0;32m     83\u001b[0m \u001b[38;5;66;03m# Run the Main Program\u001b[39;00m\n\u001b[0;32m     84\u001b[0m \u001b[38;5;66;03m# ---------------------------------------------------------------------\u001b[39;00m\n\u001b[0;32m     85\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;18m__name__\u001b[39m \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m__main__\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[1;32m---> 86\u001b[0m     \u001b[43mmain\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[1;32mIn[10], line 49\u001b[0m, in \u001b[0;36mmain\u001b[1;34m()\u001b[0m\n\u001b[0;32m     46\u001b[0m \u001b[38;5;66;03m# 5. Run Inference on a Test Image\u001b[39;00m\n\u001b[0;32m     47\u001b[0m \u001b[38;5;66;03m# Assuming you have test image, bboxes, CNN model, SVMs, transform, and classes\u001b[39;00m\n\u001b[0;32m     48\u001b[0m test_image_path \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtest_image.jpg\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m---> 49\u001b[0m test_image \u001b[38;5;241m=\u001b[39m \u001b[43mskimage\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mio\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mimread\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtest_image_path\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;66;03m#i need a BGR in order to apply selective search\u001b[39;00m\n\u001b[0;32m     51\u001b[0m test_bboxes \u001b[38;5;241m=\u001b[39m selective_search_proposal(test_image) \u001b[38;5;66;03m#get all the test_bboxes.\u001b[39;00m\n\u001b[0;32m     53\u001b[0m detections \u001b[38;5;241m=\u001b[39m predict(test_image_path, test_bboxes, cnn_model, svms, transform, class_names, iou_threshold\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.3\u001b[39m, confidence_threshold\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.5\u001b[39m)\n",
      "File \u001b[1;32mc:\\Users\\Debojyoti Das\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\skimage\\_shared\\utils.py:328\u001b[0m, in \u001b[0;36mdeprecate_parameter.__call__.<locals>.fixed_func\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    324\u001b[0m     \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnew_name \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    325\u001b[0m         \u001b[38;5;66;03m# Assign old value to new one\u001b[39;00m\n\u001b[0;32m    326\u001b[0m         kwargs[\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnew_name] \u001b[38;5;241m=\u001b[39m deprecated_value\n\u001b[1;32m--> 328\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\Debojyoti Das\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\skimage\\io\\_io.py:82\u001b[0m, in \u001b[0;36mimread\u001b[1;34m(fname, as_gray, plugin, **plugin_args)\u001b[0m\n\u001b[0;32m     79\u001b[0m         plugin \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtifffile\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[0;32m     81\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m file_or_url_context(fname) \u001b[38;5;28;01mas\u001b[39;00m fname, _hide_plugin_deprecation_warnings():\n\u001b[1;32m---> 82\u001b[0m     img \u001b[38;5;241m=\u001b[39m \u001b[43mcall_plugin\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mimread\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfname\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mplugin\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mplugin\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mplugin_args\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     84\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(img, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mndim\u001b[39m\u001b[38;5;124m'\u001b[39m):\n\u001b[0;32m     85\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m img\n",
      "File \u001b[1;32mc:\\Users\\Debojyoti Das\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\skimage\\_shared\\utils.py:538\u001b[0m, in \u001b[0;36mdeprecate_func.__call__.<locals>.wrapped\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    536\u001b[0m stacklevel \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mget_stack_length(func) \u001b[38;5;241m-\u001b[39m stack_rank\n\u001b[0;32m    537\u001b[0m warnings\u001b[38;5;241m.\u001b[39mwarn(message, category\u001b[38;5;241m=\u001b[39m\u001b[38;5;167;01mFutureWarning\u001b[39;00m, stacklevel\u001b[38;5;241m=\u001b[39mstacklevel)\n\u001b[1;32m--> 538\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\Debojyoti Das\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\skimage\\io\\manage_plugins.py:254\u001b[0m, in \u001b[0;36mcall_plugin\u001b[1;34m(kind, *args, **kwargs)\u001b[0m\n\u001b[0;32m    251\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mIndexError\u001b[39;00m:\n\u001b[0;32m    252\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mCould not find the plugin \u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mplugin\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m for \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mkind\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m--> 254\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\Debojyoti Das\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\skimage\\io\\_plugins\\imageio_plugin.py:11\u001b[0m, in \u001b[0;36mimread\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m      9\u001b[0m \u001b[38;5;129m@wraps\u001b[39m(imageio_imread)\n\u001b[0;32m     10\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mimread\u001b[39m(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m---> 11\u001b[0m     out \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39masarray(\u001b[43mimageio_imread\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[0;32m     12\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m out\u001b[38;5;241m.\u001b[39mflags[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mWRITEABLE\u001b[39m\u001b[38;5;124m'\u001b[39m]:\n\u001b[0;32m     13\u001b[0m         out \u001b[38;5;241m=\u001b[39m out\u001b[38;5;241m.\u001b[39mcopy()\n",
      "File \u001b[1;32mc:\\Users\\Debojyoti Das\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\imageio\\v3.py:53\u001b[0m, in \u001b[0;36mimread\u001b[1;34m(uri, index, plugin, extension, format_hint, **kwargs)\u001b[0m\n\u001b[0;32m     50\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m index \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m     51\u001b[0m     call_kwargs[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mindex\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m index\n\u001b[1;32m---> 53\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[43mimopen\u001b[49m\u001b[43m(\u001b[49m\u001b[43muri\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mr\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mplugin_kwargs\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mas\u001b[39;00m img_file:\n\u001b[0;32m     54\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m np\u001b[38;5;241m.\u001b[39masarray(img_file\u001b[38;5;241m.\u001b[39mread(\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mcall_kwargs))\n",
      "File \u001b[1;32mc:\\Users\\Debojyoti Das\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\imageio\\core\\imopen.py:113\u001b[0m, in \u001b[0;36mimopen\u001b[1;34m(uri, io_mode, plugin, extension, format_hint, legacy_mode, **kwargs)\u001b[0m\n\u001b[0;32m    111\u001b[0m     request\u001b[38;5;241m.\u001b[39mformat_hint \u001b[38;5;241m=\u001b[39m format_hint\n\u001b[0;32m    112\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m--> 113\u001b[0m     request \u001b[38;5;241m=\u001b[39m \u001b[43mRequest\u001b[49m\u001b[43m(\u001b[49m\u001b[43muri\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mio_mode\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mformat_hint\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mformat_hint\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mextension\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mextension\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    115\u001b[0m source \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m<bytes>\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(uri, \u001b[38;5;28mbytes\u001b[39m) \u001b[38;5;28;01melse\u001b[39;00m uri\n\u001b[0;32m    117\u001b[0m \u001b[38;5;66;03m# fast-path based on plugin\u001b[39;00m\n\u001b[0;32m    118\u001b[0m \u001b[38;5;66;03m# (except in legacy mode)\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\Debojyoti Das\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\imageio\\core\\request.py:249\u001b[0m, in \u001b[0;36mRequest.__init__\u001b[1;34m(self, uri, mode, extension, format_hint, **kwargs)\u001b[0m\n\u001b[0;32m    246\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mInvalid Request.Mode: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mmode\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m    248\u001b[0m \u001b[38;5;66;03m# Parse what was given\u001b[39;00m\n\u001b[1;32m--> 249\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_parse_uri\u001b[49m\u001b[43m(\u001b[49m\u001b[43muri\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    251\u001b[0m \u001b[38;5;66;03m# Set extension\u001b[39;00m\n\u001b[0;32m    252\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m extension \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "File \u001b[1;32mc:\\Users\\Debojyoti Das\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\imageio\\core\\request.py:409\u001b[0m, in \u001b[0;36mRequest._parse_uri\u001b[1;34m(self, uri)\u001b[0m\n\u001b[0;32m    406\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m is_read_request:\n\u001b[0;32m    407\u001b[0m     \u001b[38;5;66;03m# Reading: check that the file exists (but is allowed a dir)\u001b[39;00m\n\u001b[0;32m    408\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mexists(fn):\n\u001b[1;32m--> 409\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mFileNotFoundError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mNo such file: \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m%\u001b[39m fn)\n\u001b[0;32m    410\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    411\u001b[0m     \u001b[38;5;66;03m# Writing: check that the directory to write to does exist\u001b[39;00m\n\u001b[0;32m    412\u001b[0m     dn \u001b[38;5;241m=\u001b[39m os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mdirname(fn)\n",
      "\u001b[1;31mFileNotFoundError\u001b[0m: No such file: 'c:\\Users\\Debojyoti Das\\Downloads\\History\\Rich feature hierarchies for accurate object detection and semantic segmentation\\test_image.jpg'"
     ]
    }
   ],
   "source": [
    "def main():\n",
    "    # 0. Setup variables\n",
    "    image_path = \"example.jpg\" #example image\n",
    "    annotation_dir=\"./Annotations/\"\n",
    "\n",
    "    #create class and bboxes as it appears in annotations folder\n",
    "    class_names = [\"Aeroplane\", \"Bicycle\", \"Bird\", \"Boat\", \"Bottle\", \"Bus\", \"Car\", \"Cat\", \"Chair\", \"Cow\", \"Diningtable\", \"Dog\", \"Horse\", \"Motorbike\", \"Person\", \"Pottedplant\", \"Sheep\", \"Sofa\", \"Train\",\"Tvmonitor\"] #PASCAL VOC classes from example\n",
    "    bboxes={} #bboxes[\"image_name\"] will contain proposal bboxes\n",
    "\n",
    "    #1. Make necessary folders to do all those things.\n",
    "    image_dir=\"./JPEGImages/\"\n",
    "    if not os.path.exists(image_dir):\n",
    "        os.makedirs(image_dir)\n",
    "        #exit() #I added this part. so you don't need to keep training for multiple times. if model is there, it loads and it's done. saves you tremendous amount of time.\n",
    "\n",
    "    annotation_dir=\"./Annotations/\"\n",
    "    if not os.path.exists(annotation_dir):\n",
    "        os.makedirs(annotation_dir)\n",
    "        #exit() #I added this part. so you don't need to keep training for multiple times. if model is there, it loads and it's done. saves you tremendous amount of time.\n",
    "\n",
    "    #Now make a dummy XML folder so that GT Bounding Box are called during the training loop\n",
    "    for img_file in os.listdir(image_dir):\n",
    "        base_name = os.path.basename(img_file).split('.')[0]\n",
    "        bboxes[base_name] = [] #create an image\n",
    "        bboxes[base_name] = selective_search_proposal(skimage.io.imread(image_dir+img_file)) #put selective search to it.\n",
    "\n",
    "\n",
    "    # 1. Load Image (placeholder)\n",
    "    # ... Load your image and annotation data ...\n",
    "    # For the example, let's assume loading bboxes and setting class_names\n",
    "\n",
    "    # 2. Initialize CNN Feature Extractor\n",
    "    transform = transforms.Compose([\n",
    "        transforms.Resize((227, 227)),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
    "    ])\n",
    "    cnn_model = CNNFeatureExtractor(use_cuda=True)\n",
    "\n",
    "    # 3. Create Training Data\n",
    "    X, y = create_training_data(image_dir, annotation_dir, bboxes, class_names, lambda img,bb: extract_features(img, bb, cnn_model, transform))\n",
    "\n",
    "    #4. Train SVMs\n",
    "    svms = train_svms(X, y, class_names)\n",
    "\n",
    "    # 5. Run Inference on a Test Image\n",
    "    # Assuming you have test image, bboxes, CNN model, SVMs, transform, and classes\n",
    "    test_image_path = \"test_image.jpg\"\n",
    "    test_image = skimage.io.imread(test_image_path) #i need a BGR in order to apply selective search\n",
    "\n",
    "    test_bboxes = selective_search_proposal(test_image) #get all the test_bboxes.\n",
    "\n",
    "    detections = predict(test_image_path, test_bboxes, cnn_model, svms, transform, class_names, iou_threshold=0.3, confidence_threshold=0.5)\n",
    "\n",
    "    # Print the results\n",
    "    for det in detections:\n",
    "        print(f\"Detected: {det['class']} with score {det['score']} at {det['bbox']}\")\n",
    "\n",
    "\n",
    "# ---------------------------------------------------------------------\n",
    "# Helper Functions (Implement these based on your data format)\n",
    "# ---------------------------------------------------------------------\n",
    "\n",
    "def load_annotations(annotation_path):\n",
    "    \"\"\"\n",
    "    Loads object bounding boxes and labels from an annotation file (PASCAL VOC XML).\n",
    "\n",
    "    Args:\n",
    "        annotation_path: Path to the annotation file.\n",
    "\n",
    "    Returns:\n",
    "        A tuple (boxes, labels) where boxes is a list of (x1, y1, x2, y2) bounding boxes\n",
    "        and labels is a list of corresponding class names.\n",
    "    \"\"\"\n",
    "    # Implement loading XML annotations from PASCAL VOC format\n",
    "    # ... (Use libraries like xml.etree.ElementTree)\n",
    "    boxes=[]\n",
    "    labels=[] #get gt labels and bounding boxes\n",
    "    return boxes, labels\n",
    "\n",
    "\n",
    "# ---------------------------------------------------------------------\n",
    "# Run the Main Program\n",
    "# ---------------------------------------------------------------------\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
